{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d361ee40-c93b-44c9-89b3-70ff8dbb844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python environment details saved to ../Paper_Figures/Package_Info/pythonStep3_environment.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "import shap\n",
    "from GenerateSHAPFuncs import *\n",
    "import matplotlib.pyplot as plt \n",
    "import xgboost\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#write versions\n",
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "# Get the Python version\n",
    "python_version = sys.version\n",
    "\n",
    "# Get the list of installed packages with versions\n",
    "installed_packages = sorted([\"{}=={}\".format(d.project_name, d.version) for d in pkg_resources.working_set])\n",
    "\n",
    "# Define output file\n",
    "output_file = \"../Paper_Figures/Package_Info/pythonStep3_environment.txt\"\n",
    "\n",
    "# Write to file\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(f\"Python Version:\\n{python_version}\\n\\n\")\n",
    "    f.write(\"Installed Packages:\\n\")\n",
    "    f.write(\"\\n\".join(installed_packages))\n",
    "\n",
    "print(f\"Python environment details saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c75614c9-9193-4d95-96c4-1c49a2a9796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: FM\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brettonb/.local/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [18:10:32] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"deterministic\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# This code chunk will produce the FM dataset used in the paper. This chunk is self contained and does not need GenerateSHAPFuncs.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Clean Column Name\n",
    "# ---------------------------\n",
    "def clean_column_name(name):\n",
    "    \"\"\"\n",
    "    Clean a column name by:\n",
    "      - Converting to a string.\n",
    "      - Replacing any non-alphanumeric character (except underscore) with an underscore.\n",
    "      - Prefixing with 'f_' if the name starts with a digit.\n",
    "      - Using a default name if empty.\n",
    "    \"\"\"\n",
    "    name = str(name)\n",
    "    # Replace non-alphanumeric characters (except underscore) with underscore\n",
    "    name = re.sub(r'[^0-9a-zA-Z_]+', '_', name)\n",
    "    if not name:\n",
    "        name = \"col\"\n",
    "    if name[0].isdigit():\n",
    "        name = \"f_\" + name\n",
    "    return name\n",
    "\n",
    "# ---------------------------\n",
    "# Function: Add Median Lifespan Increase\n",
    "# ---------------------------\n",
    "def add_median_lifespan_increase(data):\n",
    "    data = data.copy()\n",
    "    data['median_lifespan_increase'] = np.nan  # Initialize with NaN\n",
    "\n",
    "    # Define mapping from 'Grp_Sex' to 'median_lifespan_increase'\n",
    "    mapping = {\n",
    "        \"M_Rapa\": 23,\n",
    "        \"M_Cana\": 14,\n",
    "        \"M_17aE2\": 12,\n",
    "        \"M_CR\": 32,\n",
    "        \"M_Aca\": 22,\n",
    "        \"M_Cont_12\": 0\n",
    "    }\n",
    "    data['median_lifespan_increase'] = data['Grp_Sex'].map(mapping)\n",
    "    return data\n",
    "\n",
    "# ---------------------------\n",
    "# Function: Train Model & Compute SHAP (Using XGBoost)\n",
    "# ---------------------------import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def train_and_compute_shap(dataset, dataset_name, selected_groups, output_dir='./', additional_exclude_cols=None):\n",
    "    # Prepare the dataset\n",
    "    dataset = add_median_lifespan_increase(dataset)\n",
    "    \n",
    "    # Filter dataset for selected groups\n",
    "    dataset = dataset[dataset['Grp_Sex'].isin(selected_groups)].reset_index(drop=True)\n",
    "    \n",
    "    # Default columns to exclude\n",
    "    default_exclude_cols = [\n",
    "        \"median_lifespan_increase\", \"Grp_Sex\", \"Lifespan_Increased2\", \"Grp\",\n",
    "        \"Mouse\", \"ID\", \"group\", \"X\", \"Treatment\", \"X18198\", \"Unnamed: 0\"\n",
    "    ]\n",
    "    \n",
    "    # Combine exclude lists\n",
    "    if additional_exclude_cols is not None:\n",
    "        exclude_cols = default_exclude_cols + additional_exclude_cols\n",
    "    else:\n",
    "        exclude_cols = default_exclude_cols\n",
    "\n",
    "    # Get predictor variables\n",
    "    predictor_vars_original = [col for col in dataset.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Convert predictor variables to numeric\n",
    "    dataset_numeric = dataset[predictor_vars_original].apply(pd.to_numeric, errors='coerce')\n",
    "    valid_cols = dataset_numeric.columns[~dataset_numeric.isin([np.nan, np.inf, -np.inf]).any()]\n",
    "    dataset_numeric = dataset_numeric[valid_cols]\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    dataset_numeric = dataset_numeric.dropna()\n",
    "    dataset = dataset.loc[dataset_numeric.index].reset_index(drop=True)\n",
    "    dataset_numeric = dataset_numeric.reset_index(drop=True)\n",
    "    \n",
    "    # Create a mapping from cleaned names to original\n",
    "    cleaned_names = [clean_column_name(col) for col in dataset_numeric.columns]\n",
    "    mapping_dict = dict(zip(cleaned_names, dataset_numeric.columns))\n",
    "    dataset_numeric.columns = cleaned_names  # Rename dataset\n",
    "\n",
    "    # Train XGBoost Model\n",
    "   # 'subsample': 0.8, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.6},\n",
    "    xgb_model = XGBRegressor(n_estimators=500,\n",
    "                             random_state=42,\n",
    "                             subsample =0.8,\n",
    "                             learning_rate=0.2,\n",
    "                             max_depth = 3,\n",
    "                             colsample_bytree =0.6,\n",
    "                             n_jobs=-1,\n",
    "                            deterministic=True)\n",
    "    xgb_model.fit(dataset_numeric, dataset['median_lifespan_increase'])\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(dataset_numeric)\n",
    "\n",
    "    # **Extract SHAP's Default Sorting**\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)  # Mean absolute SHAP values\n",
    "    shap_default_order = np.argsort(mean_abs_shap)[::-1]  # Sort in descending order\n",
    "\n",
    "    # Extract ordered feature names **using SHAP's default sorting**\n",
    "    ordered_cleaned = np.array(dataset_numeric.columns)[shap_default_order]  # Ordered feature names\n",
    "    ordered_pretty = [mapping_dict.get(col, col) for col in ordered_cleaned]  # Map back to original names\n",
    "\n",
    "    # **Save the exact SHAP default sorting to a CSV file**\n",
    "    order_df = pd.DataFrame({'Order': range(1, len(ordered_pretty) + 1), 'Feature': ordered_pretty})\n",
    "    order_df.to_csv(os.path.join(output_dir, f\"SHAP_Default_Feature_Order_{dataset_name}.csv\"), index=False)\n",
    "\n",
    "    # **Force the SHAP summary plot to use this exact order**\n",
    "    ordered_dataset_numeric = dataset_numeric[ordered_cleaned]\n",
    "    pretty_feature_names = ordered_pretty  # Ensure SHAP uses the original names in the plot\n",
    "\n",
    "    # **Save SHAP summary plot using the extracted default sorting**\n",
    "    plt.figure()\n",
    "    shap.summary_plot(\n",
    "        shap_values[:, shap_default_order],  # Use SHAPâ€™s default sorting\n",
    "        ordered_dataset_numeric,\n",
    "        feature_names=pretty_feature_names,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP Summary Plot for {dataset_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"SHAP_Summary_{dataset_name}.png\"))\n",
    "    plt.savefig(os.path.join(output_dir, f\"SHAP_Summary_{dataset_name}.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "    return order_df  # Return order for debugging if needed\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Script\n",
    "# ---------------------------\n",
    "# Define the groups to analyze\n",
    "selected_groups = [\"M_Cana\", \"M_Aca\", \"M_Rapa\", \"M_17aE2\", \"M_CR\", \"M_Cont_12\"]\n",
    "\n",
    "# Specify the output directory for results and SHAP files\n",
    "output_directory = \"../Paper_Figures\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize a DataFrame to collect all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Define additional columns to exclude (adjust these as needed)\n",
    "additional_exclude_cols = ['Mouse', 'mouse', 'Condition', 'Treatment', 'Unmaed:0']\n",
    "\n",
    "# Assume that 'datasets' is a dictionary of your datasets, for example:\n",
    "# datasets = {\n",
    "#     'FM': pd.read_csv('FM.csv'),\n",
    "#     'AM': pd.read_csv('AM.csv'),\n",
    "#     ... \n",
    "# }\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\\n\")\n",
    "    try:\n",
    "        results = train_and_compute_shap(\n",
    "            dataset=dataset,\n",
    "            dataset_name=dataset_name,\n",
    "            selected_groups=selected_groups,\n",
    "            output_dir=output_directory,\n",
    "            additional_exclude_cols=additional_exclude_cols\n",
    "        )\n",
    "    \n",
    "        if results is not None:\n",
    "            all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {dataset_name}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "\n",
    "# Optionally, save the concatenated results\n",
    "all_results.to_csv(os.path.join(output_directory, \"All_Results_Combined.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5581684b-fb68-4be9-9e69-6d90d8bc0b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python environment details saved to ../Paper_Figures/Package_Info/pythonStep3_environment.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a3f88b-b6e5-4ebf-ad99-3b4fa588d100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: FM\n",
      "\n",
      "Number of samples in FM: 94\n",
      "Number of valid predictor variables: 1051\n",
      "Groups in dataset: M_Aca, M_Cont_12, M_Cana, M_CR, M_17aE2, M_Rapa\n",
      "Unique values in median_lifespan_increase:\n",
      "[22.  0. 14. 32. 12. 23.]\n",
      "Error processing dataset: FM\n",
      "Error message: feature_names must be string, and may not contain [, ] or <\n",
      "\n",
      "Final concatenated results:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#This broke for some reason. I have no idea why. The code above will work for the paper, this was cool when it was working.\n",
    "import os\n",
    "import pandas as pd\n",
    "from GenerateSHAPFuncs import train_and_compute_shap\n",
    "\n",
    "# =============================\n",
    "# 5) Example Usage\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    AM = pd.read_csv('../Step1_LoadingAndCleaningData/AM.csv')\n",
    "    FM = pd.read_csv('../Step1_LoadingAndCleaningData/FM.csv')\n",
    "    AP = pd.read_csv('../Step1_LoadingAndCleaningData/AP.csv')\n",
    "    FP = pd.read_csv('../Step1_LoadingAndCleaningData/FP.csv')\n",
    "    AMP = pd.read_csv('../Step1_LoadingAndCleaningData/AMP.csv')\n",
    "    FMP = pd.read_csv('../Step1_LoadingAndCleaningData/FMP.csv')\n",
    "\n",
    "    datasets = {\n",
    "        'FM': FM\n",
    "     #   'AM': AM,\n",
    "    #    'AP': AP,\n",
    "   #     'FP': FP,\n",
    "  #      'FMP': FMP,\n",
    " #       'AMP': AMP\n",
    "    }\n",
    "\n",
    "    selected_groups = [\"M_Cana\", \"M_Aca\", \"M_Rapa\", \"M_17aE2\", \"M_CR\", \"M_Cont_12\"]\n",
    "    output_directory = \"ShapOutput\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "\n",
    "    additional_exclude_cols = [\n",
    "        'Grp_Sex', 'Lifespan_Increased2', 'Grp', \n",
    "        'Mouse', 'ID', 'group', 'Treatment', 'X18198', 'Condition', 'Sex', \n",
    "        'Mouse_ID', 'Unnamed: 0'\n",
    "    ]\n",
    "\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\\n\")\n",
    "        try:\n",
    "            results = train_and_compute_shap(\n",
    "                dataset=dataset,\n",
    "                dataset_name=dataset_name,\n",
    "                selected_groups=selected_groups,\n",
    "                output_dir=output_directory,\n",
    "                additional_exclude_cols=additional_exclude_cols\n",
    "            )\n",
    "        \n",
    "            if results is not None:\n",
    "                all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset: {dataset_name}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "\n",
    "    # Optionally inspect or save all_results\n",
    "    print(\"\\nFinal concatenated results:\")\n",
    "    print(all_results.head())\n",
    "    all_results.to_csv(os.path.join(output_directory, \"All_Results_Combined.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
